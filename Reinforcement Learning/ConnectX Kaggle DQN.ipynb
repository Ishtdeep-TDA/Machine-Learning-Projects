{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# keras model approach\n",
    "from tensorflow.keras import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dense, Input\n",
    "from tensorflow.keras.activations import relu\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "print(dir(env))\n",
    "#print(env.state[0]['observation']['board'])\n",
    "'''env.state[0]['observation']['board'][0] = 1\n",
    "env.state[0]['observation']['board'][1] = 1\n",
    "env.state[0]['observation']['board'][2] = 1\n",
    "env.state[0]['observation']['board'][3] = 1'''\n",
    "print(env.done)\n",
    "env.step([1,1])\n",
    "env.step([1,1])\n",
    "a = env.step([1,1])\n",
    "print(\"A\")\n",
    "print(a)\n",
    "print(env.state[0]['observation']['board'])\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_space = env.configuration['columns']\n",
    "obs = env.reset()\n",
    "# creating a non CNN model with directly feeding all the squares to the NN\n",
    "# The input has *3 because we want each piece to have a different class,\n",
    "# so basically 3 matrices will be formed 1 for empty squares, 1 matrice for \n",
    "# player 1's coin, 1 matrice for player 2's coin \n",
    "# all will be concatenated and then fed into the NN\n",
    "\n",
    "nn_input = len(obs[0]['observation']['board'])*3\n",
    "print(nn_input)\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_keras():\n",
    "    \n",
    "    inputs = Input(shape=(nn_input,))\n",
    "    x = Dense(1000,activation='relu',kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(action_space,activation='linear',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    model = Model(inputs=inputs, outputs=output, name=\"RL_Value_Function\")\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=adam,loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model\n",
    "# converts observation into NN input type\n",
    "def encode(board):\n",
    "    empty = []\n",
    "    player_1 = []\n",
    "    player_2 = []\n",
    "    \n",
    "    for i in board:\n",
    "        if i == 0:\n",
    "            empty.append(1)\n",
    "        else:\n",
    "            empty.append(0)\n",
    "        if i == 1:\n",
    "            player_1.append(1)\n",
    "        else:\n",
    "            player_1.append(0)\n",
    "        if i == 2:\n",
    "            player_2.append(1)\n",
    "        else:\n",
    "            player_2.append(0)\n",
    "            \n",
    "    output = np.concatenate((np.array(empty),np.array(player_1),np.array(player_2)),axis=0)\n",
    "    return output\n",
    "model_1 = model_keras()\n",
    "model_2 = model_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class ConnectX(gym.Env):\n",
    "    def __init__(self, switch_prob=0.0):\n",
    "        self.env = make('connectx', debug=False)\n",
    "        self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.switch_prob = switch_prob\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def switch_trainer(self):\n",
    "        self.pair = self.pair[::-1]\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        if np.random.random() < self.switch_prob:\n",
    "            self.switch_trainer()\n",
    "        return self.trainer.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true,y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets build some memory into the model to perform decorrelated batch updates\n",
    "# this is TD learning\n",
    "\n",
    "# so apparantly the NN has to be adjusted only according to the action taken by it\n",
    "# for example if action 2 is taken then only the weights for action 2 should be changed\n",
    "# for this reason we should only update the q_2 vector with regarding the chosen action\n",
    "# rewards will also be added to that action only\n",
    "# q_2 is what we thought the value of the state will be after doing action\n",
    "# we also add the reward and make this the target for the NN\n",
    "\n",
    "replay_batch = deque(maxlen = 3000)\n",
    "\n",
    "warmup = 10 #will start training after these many episodes have passed\n",
    "\n",
    "# to balance exploration\n",
    "epsilon = {\n",
    "\"epsilon\" : 1.0,\n",
    "\"epsilon_decay\": 0.99999,\n",
    "\"epsilon_min\":0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2020)\n",
    "\n",
    "#@tf.function\n",
    "def batch_train(model_1,model_2,gamma,batch_size,epsilon):\n",
    "    \n",
    "    #decaying the exploration\n",
    "    if epsilon['epsilon'] > epsilon['epsilon_min']:\n",
    "         epsilon['epsilon'] =  epsilon['epsilon'] * epsilon['epsilon_decay']\n",
    "   \n",
    "    batch = random.sample(replay_batch,batch_size)\n",
    "    \n",
    "    batch_reward = []\n",
    "    batch_action = []\n",
    "    batch_done = []\n",
    "    \n",
    "    batch_current_state = np.zeros((batch_size, nn_input))\n",
    "    batch_next_state = np.zeros((batch_size, nn_input))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        batch_reward.append(batch[i][2])\n",
    "        batch_action.append(batch[i][1])\n",
    "        batch_current_state[i] = batch[i][0]\n",
    "        batch_next_state[i] = batch[i][3]\n",
    "        batch_done.append(batch[i][4])\n",
    "    #lets calculate the next state value as the current value will be calculated in \n",
    "    # in gradient tape\n",
    "    \n",
    "    next_q = model_2.predict(batch_next_state)\n",
    "    \n",
    "    max_q = []\n",
    "    for i in next_q:\n",
    "        max_q.append(max(i))\n",
    "    max_q = np.array(max_q,dtype = 'float32')\n",
    "    \n",
    "    target = batch_reward + gamma*max_q # this is the Q learning Target\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # logits is the forward pass\n",
    "        logits = model_1(batch_current_state, training=True)\n",
    "        \n",
    "        q_target = np.array(logits)\n",
    "        \n",
    "        \n",
    "        # VERY IMPORTANT NOTE, IF THE EPISODE ENDS THE DONE VALUE BECOMES TRUE\n",
    "        # IT IS VERY IMPORTANT THAT THE NN UPDATES TOWARDS THIS TRUE VALUE RATHER THAN\n",
    "        # ITS OWN THINKING VALUE (r + gamma*max(action)) THAT WE USE FOR ALL\n",
    "        # NON TERMINAL REWARDS \n",
    "        # THIS MAKES OR BREAKS THE NETWORK VERY VERY IMPORTANT\n",
    "        for i in range(batch_size):\n",
    "            q_target[i][batch_action[i]] = target[i]\n",
    "            if batch_done[i]:\n",
    "                q_target[i][batch_action[i]] = batch_reward[i]\n",
    "\n",
    "        # calculating the loss\n",
    "        loss_value = custom_loss(q_target,logits)\n",
    "    \n",
    "    #we retrieve the gradients\n",
    "    grads = tape.gradient(loss_value, model_1.trainable_weights)\n",
    "    \n",
    "    #THIS IS ONE STEP OF GRAD DESCENT (Minimizes the loss)\n",
    "    adam.apply_gradients(zip(grads, model_1.trainable_weights))\n",
    "\n",
    "def policy(q_vals,turn):\n",
    "    # lets implement a policy which decays\n",
    "    if np.random.rand() <= epsilon['epsilon']:  \n",
    "        return random.randrange(action_space)\n",
    "    elif turn == True:\n",
    "        action = np.argmax(q_vals[0])\n",
    "        return action\n",
    "    else:\n",
    "        action = np.argmin(q_vals[0])\n",
    "        return action\n",
    "def update_target_network():\n",
    "    model_2.set_weights(model_1.get_weights())\n",
    "# this is the custom reward function\n",
    "def get_reward(rew):\n",
    "    reward = None\n",
    "    if rew == 1:\n",
    "        reward = 1\n",
    "    elif rew == -1:\n",
    "        reward = -1\n",
    "    elif rew == None:\n",
    "        reward = -5\n",
    "    else:\n",
    "        reward = 0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "x = env.step([0,1])\n",
    "x = env.step([0,1])\n",
    "x = env.step([0,1])\n",
    "x = env.step([0,1])\n",
    "x = env.step([0,1])\n",
    "x = env.step([0,1])\n",
    "x = env.step([0,1])\n",
    "print(env.done)\n",
    "print(x)\n",
    "print(encode(x[0]['observation']['board']))\n",
    "env.render(mode=\"ipython\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THE TRAINING LOOP\n",
    "global_steps = 0\n",
    "# to have same networks in the starting\n",
    "update_target_network()\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    observation = env.reset()[0]['observation']['board']\n",
    "    observation = encode(observation)\n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    done = False\n",
    "    turn = True\n",
    "    total_reward = 0\n",
    "    while not done:        \n",
    "        #storing the current state\n",
    "        state_1 = observation\n",
    "        # this is the current q values\n",
    "        q_state = model_1(observation,training = False)\n",
    "        \n",
    "        action = policy(q_state,turn) # current action\n",
    "        \n",
    "        # ok so this environments works a bit different \n",
    "        # actions of player 1 have to be in the format ['player 1 action',0]\n",
    "        # actions of player 2 havet to be in the format [0,'player 2']\n",
    "        st = None\n",
    "        if turn == True:\n",
    "            st = env.step([int(action),0])\n",
    "            turn = False\n",
    "        else:\n",
    "            st = env.step([0,int(action)])\n",
    "            turn = True\n",
    "        \n",
    "        next_state = st[0]['observation']['board']\n",
    "        next_state = encode(next_state)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        reward = get_reward(st[0]['reward'])\n",
    "        done = env.done\n",
    "        \n",
    "        # calculating the total reward\n",
    "        total_reward = total_reward + reward\n",
    "\n",
    "        state_2 = next_state\n",
    "        state_reward = reward\n",
    "        # only store for player 1\n",
    "        if turn == True:\n",
    "            replay_batch.append((state_1,action,state_reward,state_2,done))\n",
    "        \n",
    "        observation = next_state\n",
    "        \n",
    "        if i>warmup:\n",
    "            batch_train(model_1,model_2,0.99,64,epsilon)\n",
    "            global_steps = global_steps + 1\n",
    "            \n",
    "        if done:\n",
    "            update_target_network()\n",
    "            \n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save(\"G:\\Data Science\\Reinforcement Learning\\Connect X kaggle/model_1_1000ep_DQN_ONLY.h5\")\n",
    "model_2.save(\"G:\\Data Science\\Reinforcement Learning\\Connect X kaggle/model_2_1000ep_DQN_ONLY.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST TESTING THE NETWORK\n",
    "\n",
    "for i in tqdm(range(50)):\n",
    "    observation = env.reset()[0]['observation']['board']\n",
    "    observation = encode(observation)\n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    done = False\n",
    "    turn = True\n",
    "    total_reward = 0\n",
    "    while not done:        \n",
    "        #storing the current state\n",
    "        state_1 = observation\n",
    "        # this is the current q values\n",
    "        q_state = model_1(observation,training = False)\n",
    "        action = policy(q_state,turn) # current action\n",
    "        print(\"q state\")\n",
    "        print(q_state)\n",
    "        # ok so this environments works a bit different \n",
    "        # actions of player 1 have to be in the format ['player 1 action',0]\n",
    "        # actions of player 2 havet to be in the format [0,'player 2']\n",
    "        st = None\n",
    "        if turn == True:\n",
    "            st = env.step([int(action),0])\n",
    "            turn = False\n",
    "        else:\n",
    "            st = env.step([0,int(action)])\n",
    "            turn = True\n",
    "        \n",
    "        next_state = st[0]['observation']['board']\n",
    "        next_state = encode(next_state)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        reward = get_reward(st[0]['reward'])\n",
    "\n",
    "        done = env.done\n",
    "        \n",
    "        # calculating the total reward\n",
    "        total_reward = total_reward + reward\n",
    "\n",
    "        state_2 = next_state\n",
    "        state_reward = reward\n",
    "        # only store for player 1's move\n",
    "        observation = next_state\n",
    "    env.render(mode=\"ipython\", width=300, height=300)\n",
    "    print(total_reward)\n",
    "    print(\"episode ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([None, \"negamax\"], width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}