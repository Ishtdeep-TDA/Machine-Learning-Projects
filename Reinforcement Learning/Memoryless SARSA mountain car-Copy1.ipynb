{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras model approach\n",
    "from tensorflow.keras import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dense, Input\n",
    "from tensorflow.keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING THE CODE SO THAT TENSORFLOW DOES NOT EAT THE WHOLE GPU MEMORY\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_keras():\n",
    "    \n",
    "    inputs = Input(shape=(2,))\n",
    "    \n",
    "    x = Dense(100,activation='relu',kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100,activation='relu',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100,activation='relu',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100,activation='relu',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output = Dense(3,activation='linear',kernel_initializer=\"glorot_uniform\")(x)\n",
    "    model = Model(inputs=inputs, outputs=output, name=\"RL_Value_Function\")\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    #model.compile(optimizer=adam,loss='mean_squared_error',metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "sample_model = model_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss(y_true,y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[1,2],[4,2],[5,1]])\n",
    "z[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warmup = 10 #will start training after these many episodes have passed\n",
    "training_count = 0 # this is a counter which is updated each time batch training is done\n",
    "# after certain number of batches we remove the old data (in the starting of the list)\n",
    "\n",
    "# to balance exploration\n",
    "epsilon = {\n",
    "\"epsilon\" : 1.0,\n",
    "\"epsilon_decay\": 0.999,\n",
    "\"epsilon_min\":0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2020)\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def batch_train(model,gamma,SARSA):\n",
    "    \n",
    "    #decaying the exploration\n",
    "    if epsilon['epsilon'] > epsilon['epsilon_min']:\n",
    "         epsilon['epsilon'] =  epsilon['epsilon'] * epsilon['epsilon_decay']\n",
    "    \n",
    "    curr_state = SARSA[0]\n",
    "    action = SARSA[1]\n",
    "    reward = SARSA[2]\n",
    "    next_state = SARSA[3]\n",
    "    next_action = SARSA[4]\n",
    "    q_next = model.predict(next_state)[0]\n",
    "    target = reward + q_next[next_action]*gamma\n",
    "\n",
    "    \n",
    "    done = SARSA[5]\n",
    "    if done:\n",
    "        target = reward\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # logits is the forward pass\n",
    "        logits = model(curr_state, training=True)\n",
    "        \n",
    "        q_target = np.array(logits)\n",
    "        q_target[0][action] = target\n",
    "        \n",
    "        loss_value = custom_loss(q_target,logits)\n",
    "\n",
    "    #we retrieve the gradients\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "    #THIS IS ONE STEP OF GRAD DESCENT (Minimizes the loss)\n",
    "    adam.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "def policy(q_vals,eps):\n",
    "    # lets implement a policy which decays\n",
    "    if np.random.rand() <= eps:  \n",
    "        return random.randrange(2)\n",
    "    else:\n",
    "        action = np.argmax(q_vals[0])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(1200)):\n",
    "    observation = env.reset()\n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    total_reward = 0\n",
    "    for j in range(400):\n",
    "        #storing the current state\n",
    "        state_1 = observation\n",
    "        \n",
    "        # this is the current q values\n",
    "        q_state = sample_model(observation,training = False)\n",
    "        \n",
    "        action = policy(q_state,epsilon[\"epsilon\"]) # current action\n",
    "        \n",
    "        observation,reward,done,info = env.step(action)\n",
    "        \n",
    "        # calculating the total reward\n",
    "        total_reward = total_reward + reward\n",
    "        \n",
    "        \n",
    "        #if done and j<195:\n",
    "        #    reward = -1000\n",
    "        observation = np.expand_dims(observation, axis=0)\n",
    "        state_2 = observation\n",
    "        state_reward = reward\n",
    "        \n",
    "        action_2 = policy(sample_model(observation,training = False),epsilon[\"epsilon\"])\n",
    "        \n",
    "        SARSA = (state_1,action,state_reward,state_2,action_2,done)\n",
    "\n",
    "        batch_train(sample_model,0.99,SARSA)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        if i >1150:\n",
    "            env.render()\n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets test the nn\n",
    "for i in tqdm(range(500)):\n",
    "    observation = env.reset()\n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    total_reward = 0\n",
    "    done =False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        nn_out = sample_model.predict(observation)\n",
    "        print(nn_out)\n",
    "        action = policy(nn_out,0)\n",
    "        print(action)\n",
    "        observation,reward,done,info = env.step(action)\n",
    "        observation = np.expand_dims(observation, axis=0)\n",
    "        total_reward = total_reward + reward\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random action\n",
    "for i in tqdm(range(50)):\n",
    "    observation = env.reset()\n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    for j in range(1000):\n",
    "        env.render()\n",
    "        observation,reward,done,info = env.step(env.action_space.sample())\n",
    "        print(reward,done)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
