{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "def dense(x, weights, bias, activation=tf.identity):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    z = tf.matmul(x, weights) + bias\n",
    "    return activation(z)\n",
    "\n",
    "\n",
    "def init_weights(shape, initializer):\n",
    "    \"\"\"Initialize weights for tensorflow layer.\"\"\"\n",
    "    weights = tf.Variable(\n",
    "        initializer(shape),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    \"\"\"Q-function approximator.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 action_space_size,\n",
    "                 hidden_size=[50, 50],\n",
    "                 weights_initializer=tf.initializers.glorot_uniform(),\n",
    "                 bias_initializer=tf.initializers.zeros(),\n",
    "                 optimizer=tf.optimizers.Adam,\n",
    "                 **optimizer_kwargs):\n",
    "        \"\"\"Initialize weights and hyperparameters.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.action_space_size = action_space_size\n",
    "        \n",
    "        # input will be all inputs + action taken The action taken will be represented differently for each scenario\n",
    "        # in this case we can just take all actions into account as input to the NN\n",
    "        \n",
    "        self.actual_input = input_size + action_space_size\n",
    "    \n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        np.random.seed(41)\n",
    "\n",
    "        self.initialize_weights(weights_initializer, bias_initializer)\n",
    "        self.optimizer = optimizer(**optimizer_kwargs)\n",
    "        self.update_count = 0 # this will tell how many times the train_step method is executed\n",
    "        \n",
    "    def initialize_weights(self, weights_initializer, bias_initializer):\n",
    "        \"\"\"Initialize and store weights.\"\"\"\n",
    "        wshapes = [\n",
    "            [self.actual_input, self.hidden_size[0]],\n",
    "            [self.hidden_size[0], self.hidden_size[1]],\n",
    "            [self.hidden_size[1], 1]\n",
    "        ]\n",
    "\n",
    "        bshapes = [\n",
    "            [1, self.hidden_size[0]],\n",
    "            [1, self.hidden_size[1]],\n",
    "            [1, 1]\n",
    "        ]\n",
    "\n",
    "        self.weights = [init_weights(s, weights_initializer) for s in wshapes]\n",
    "        self.biases = [init_weights(s, bias_initializer) for s in bshapes]\n",
    "\n",
    "        self.trainable_variables = self.weights + self.biases\n",
    "\n",
    "    def model(self, inputs):\n",
    "        \"\"\"Given a state vector, return the Q values of actions.\"\"\"\n",
    "        h1 = dense(inputs, self.weights[0], self.biases[0], tf.nn.relu)\n",
    "        h2 = dense(h1, self.weights[1], self.biases[1], tf.nn.relu)\n",
    "\n",
    "        out = dense(h2, self.weights[2], self.biases[2])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def train_step(self, inputs, targets, actions_one_hot):\n",
    "        \"\"\"Update weights.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            qvalues = tf.squeeze(self.model(inputs))\n",
    "            preds = tf.reduce_sum(qvalues * actions_one_hot, axis=1)\n",
    "            loss = tf.losses.mean_squared_error(targets, preds)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.update_count = self.update_count + 1\n",
    "        #print(\"NN_updated : \",self.update_count + 1)\n",
    "\n",
    "class Memory(object):\n",
    "    \"\"\"Memory buffer for Experience Replay.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"Initialize a buffer containing max_size experiences.\"\"\"\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Add an experience to the buffer.\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from the buffer.\"\"\"\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(\n",
    "            np.arange(buffer_size),\n",
    "            size=batch_size,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Interface to access buffer length.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"Deep Q-learning agent.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_space_size,\n",
    "                 action_space_size,\n",
    "                 target_update_freq=1000,\n",
    "                 discount=0.99,\n",
    "                 batch_size=32,\n",
    "                 max_explore=0.05,\n",
    "                 anneal_rate=(1 / 100000),\n",
    "                 replay_memory_size=100000,\n",
    "                 replay_start_size=10000):\n",
    "        \"\"\"Set parameters, initialize network.\"\"\"\n",
    "        self.action_space_size = action_space_size\n",
    "        # DQN has 2 networks THESE ARE STATE (V) ESTIMATORS AND NOT Q ESTIMATORS\n",
    "        # hell this is a policy gradient method it is not even a V estimator\n",
    "        self.online_network = Network(state_space_size, action_space_size)\n",
    "        self.target_network = Network(state_space_size, action_space_size)\n",
    "\n",
    "        self.update_target_network()\n",
    "\n",
    "        # training parameters\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # policy during learning\n",
    "        self.max_explore = max_explore + (anneal_rate * replay_start_size)\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.steps = 0\n",
    "\n",
    "        # replay memory\n",
    "        self.memory = Memory(replay_memory_size)\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.experience_replay = Memory(replay_memory_size)\n",
    "        \n",
    "        self.last_state = []\n",
    "        self.last_action = None\n",
    "        \n",
    "    def handle_episode_start(self):\n",
    "        self.last_state, self.last_action = [], None\n",
    "\n",
    "    def step(self, observation, training=True):\n",
    "        \"\"\"Observe state and rewards, select action.\n",
    "\n",
    "        It is assumed that `observation` will be an object with\n",
    "        a `state` vector and a `reward` float or integer. The reward\n",
    "        corresponds to the action taken in the previous step.\n",
    "        \"\"\"\n",
    "        last_state, last_action = self.last_state, self.last_action\n",
    "        last_reward = observation[1]\n",
    "        state = observation[0]\n",
    "        \n",
    "        action = self.policy(state, training)\n",
    "\n",
    "        if training:\n",
    "            self.steps += 1\n",
    "\n",
    "            if len(last_state) != 0:\n",
    "                experience = {\n",
    "                    \"state\": last_state,\n",
    "                    \"action\": last_action,\n",
    "                    \"reward\": last_reward,\n",
    "                    \"next_state\": state\n",
    "                }\n",
    "\n",
    "                self.memory.add(experience)\n",
    "\n",
    "            if self.steps > self.replay_start_size:\n",
    "                self.train_network()\n",
    "\n",
    "                if self.steps % self.target_update_freq == 0:\n",
    "                    self.update_target_network()\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        #print(\"Step Taken \", self.steps)\n",
    "        return action\n",
    "    \n",
    "    def policy(self,state, training):\n",
    "        \"\"\"Epsilon-greedy policy for training, greedy policy otherwise.\"\"\"\n",
    "        #explore_prob = self.max_explore - (self.steps * self.anneal_rate)\n",
    "        explore = max_explore > np.random.rand()\n",
    "\n",
    "        if training and explore:\n",
    "            action = np.random.randint(self.action_space_size)\n",
    "        else:\n",
    "            inputs = np.expand_dims(state, 0)\n",
    "            q_vals = []\n",
    "            for i in range(action_space_size):\n",
    "                # here i is the same as the action as in gym\n",
    "                # the actions are represented by integers\n",
    "                actions_one_hot = np.eye(self.action_space_size)[i]\n",
    "                # combining inputs and actions\n",
    "                inputs = np.append(inputs,actions_one_hot)\n",
    "                temp_q = self.online_network.model(inputs)\n",
    "                q_vals.append(temp_q)\n",
    "            action = max(q_vals)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network weights with current online network values.\"\"\"\n",
    "        variables = self.online_network.trainable_variables\n",
    "        variables_copy = [tf.Variable(v) for v in variables]\n",
    "        self.target_network.trainable_variables = variables_copy\n",
    "\n",
    "    def train_network(self):\n",
    "        \"\"\"Update online network weights.\"\"\"\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        inputs = np.array([b[\"state\"] for b in batch],dtype = 'float32')\n",
    "        actions = np.array([b[\"action\"] for b in batch])\n",
    "        rewards = np.array([b[\"reward\"] for b in batch],dtype = 'float32')\n",
    "        next_inputs = np.array([b[\"next_state\"] for b in batch],dtype = 'float32')\n",
    "        \n",
    "        actions_one_hot = np.eye(self.action_space_size)[actions]\n",
    "        actions_one_hot = np.array(actions_one_hot,dtype = 'float32')\n",
    "        \n",
    "        next_qvalues = np.squeeze(self.target_network.model(next_inputs))\n",
    "        targets = rewards + self.discount * np.amax(next_qvalues, axis=-1)\n",
    "        self.online_network.train_step(inputs, targets, actions_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "x = np.array(([1,2],[3,4])) \n",
    "\n",
    "print(x)\n",
    "print ('Array x:') \n",
    "print (x) \n",
    "print ('\\n' ) \n",
    "y = np.expand_dims(x,0) \n",
    "\n",
    "print ('Array y:') \n",
    "print (y )\n",
    "print ('\\n')\n",
    "\n",
    "print ('The shape of X and Y array:' )\n",
    "print (x.shape, y.shape )\n",
    "print ('\\n')  \n",
    "# insert axis at position 1 \n",
    "y = np.expand_dims(x, axis = 1) \n",
    "\n",
    "print ('Array Y after inserting axis at position 1:') \n",
    "print (y) \n",
    "print ('\\n')  \n",
    "\n",
    "print ('x.ndim and y.ndim:') \n",
    "print (x.ndim,y.ndim) \n",
    "print ('\\n' ) \n",
    "\n",
    "print ('x.shape and y.shape:') \n",
    "print (x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
